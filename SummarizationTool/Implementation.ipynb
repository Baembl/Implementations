{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb57975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in c:\\users\\uae\\appdata\\roaming\\python\\python312\\site-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\uae\\appdata\\roaming\\python\\python312\\site-packages (from keras) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\uae\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.0.9)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\uae\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\uae\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\UAE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install keras\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch                                                              # PyTorchd\n",
    "import pandas as pd                                                       # Pandas for data manipulation\n",
    "import seaborn as sns                                                     # Seaborn for data visualization\n",
    "import os                                                                 # OS library for file handling\n",
    "import tensorflow as tf                                                   # TensorFlow for deep learning\n",
    "import random                                                             # Random library for random number generation  \n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer                            # This is going to help us tokenize the text\n",
    "from keras.preprocessing.sequence import pad_sequences                    # This is going to help us pad the sequences\n",
    "from keras.models import Sequential                                       # This is going to help us build the model\n",
    "from textblob import TextBlob                                             # This is going to help us fix spelling mistakes in texts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer               # This is going to helps identify the most common terms in the corpus\n",
    "import re                                                                 # This library allows us to clean text data\n",
    "import nltk                                                               # Natural Language Toolkit\n",
    "nltk.download('punkt')                                                    # This divides a text into a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac420029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring Pandas to exhibit larger columns\n",
    "'''\n",
    "This is going to allow us to fully read the dialogues and their summary \n",
    "'''\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44221f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train = pd.read_csv(r\"C:\\Users\\UAE\\Downloads\\Abdur Rafay\\Datasets\\Summarization Tool Dataset\\samsum-train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\UAE\\Downloads\\Abdur Rafay\\Datasets\\Summarization Tool Dataset\\samsum-test.csv\")\n",
    "val = pd.read_csv(r\"C:\\Users\\UAE\\Downloads\\Abdur Rafay\\Datasets\\Summarization Tool Dataset\\samsum-validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cf6ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14732 entries, 0 to 14731\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        14732 non-null  object\n",
      " 1   dialogue  14731 non-null  object\n",
      " 2   summary   14732 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 345.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9a242f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna() # removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89037b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.dropna() # removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56c1f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him üôÇ\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n"
     ]
    }
   ],
   "source": [
    "print(test['dialogue'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67aadc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\UAE\\AppData\\Local\\Temp\\ipykernel_25452\\185632439.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n"
     ]
    }
   ],
   "source": [
    "def clean_tags(text):\n",
    "    clean = re.compile('<.*?>') # Compiling tags\n",
    "    clean = re.sub(clean, '', text) # Replacing tags text by an empty string\n",
    "\n",
    "    # Removing empty dialogues\n",
    "    clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n",
    "\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b179aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theresa: Hey Louise, how are u?\n",
      "Theresa: This is my workplace, they always give us so much food here üòä\n",
      "Theresa: Luckily they also offer us yoga classes, so all the food isn't much of a problem üòÇ\n",
      "Louise: Hey!! üôÇ \n",
      "Louise: Wow, that's awesome, seems great üòé Haha\n",
      "Louise: I'm good! Are you coming to visit Stockholm this summer? üôÇ\n",
      "Theresa: I don't think so :/ I need to prepare for Uni.. I will probably attend a few lessons this winter\n",
      "Louise: Nice! Do you already know which classes you will attend?\n",
      "Theresa: Yes, it will be psychology :) I want to complete a few modules that I missed :)\n",
      "Louise: Very good! Is it at the Uni in Prague?\n",
      "Theresa: No, it will be in my home town :)\n",
      "Louise: I have so much work right now, but I will continue to work until the end of summer, then I'm also back to Uni, on the 26th September!\n",
      "Theresa: You must send me some pictures, so I can see where you live :) \n",
      "Louise: I will, and of my cat and dog too ü§ó\n",
      "Theresa: Yeeeesss pls :)))\n",
      "Louise: üëåüëå\n",
      "Theresa: üê±üíï\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him üôÇ\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n"
     ]
    }
   ],
   "source": [
    "test1 = clean_tags(train['dialogue'].iloc[14727]) # Applying function to example text\n",
    "test2 = clean_tags(test['dialogue'].iloc[0]) # Applying function to example text\n",
    "\n",
    "# Printing results\n",
    "print(test1)\n",
    "print('\\n' *3)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cadb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to clean every text in the dataset.\n",
    "def clean_df(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna('').apply(clean_tags)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "152e248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning texts in all datasets\n",
    "train = clean_df(train,['dialogue', 'summary'])\n",
    "test = clean_df(test,['dialogue', 'summary'])\n",
    "val = clean_df(val,['dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4f9e5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14729</th>\n",
       "      <td>13819050</td>\n",
       "      <td>John: Every day some bad news. Japan will hunt whales again\\r\\nErica: Yes, I've read this. It's very upsetting\\r\\nJohn: Cruel Japanese\\r\\nFaith: I think this is a racist remark. Because Island and Norways has never joined this international whaling agreement\\r\\nErica: really? I haven't known, everybody is so outraged by Japan\\r\\nFaith: sure, European hypocrisy \\r\\nJohn: not entirely. Scandinavians don't use the nets that Japanese use, so Norway and Island kill much less specimens that Japan will\\r\\nFaith: oh, it's much more complex than one may expect\\r\\nJohn: True, but the truth is, that all of them should stop\\r\\nJohn: and this decision is a step back\\r\\nFaith: yes, this is worrying\\r\\nErica: And it seems that the most important whaling countries are out of the agreement right now\\r\\nFaith: yes, seems so\\r\\nJohn: Just like USA leaving the Paris Agreement</td>\n",
       "      <td>Japan is going to hunt whales again. Island and Norway never stopped hunting them. The Scandinavians kill fewer whales than the Japanese.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14730</th>\n",
       "      <td>13828395</td>\n",
       "      <td>Jennifer: Dear Celia! How are you doing?\\r\\nJennifer: The afternoon with the Collins was very pleasant, nice folks, but we missed you.\\r\\nJennifer: But I appreciate your consideration for Peter.\\r\\nCelia: My dear Jenny! It turns out that my decision not to come, though I wanted so much to see you again and Peter and the Collins, was right. Yesterday it all developed into a full bore cold. Sh.....\\r\\nCelia: All symptoms like in a text book.\\r\\nCelia: Luckily it's contagious only on the first 2, 3 days, so when we meet next week it should be alright.\\r\\nCelia: Thanks for asking! Somehow for all of us Peter comes first now.\\r\\nJennifer: That's too bad. Poor you...\\r\\nJennifer: I'll be driving to FR, do you want me to bring you sth? It's on my way.\\r\\nCelia: Thank you dear! I was at the pharmacy yesterday and had done my shopping the day before.\\r\\nCelia: You'd better still stay away from me in case I'm still contagious\\r\\nJennifer: Right. So I'll only leave a basket on your terrace. A...</td>\n",
       "      <td>Celia couldn't make it to the afternoon with the Collins and Jennifer as she is ill. She's working, but doesn't want to meet with Jennifer as it might be contagious. Jennifer will leave a basket with cookies on Celia's terrace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>13729017</td>\n",
       "      <td>Georgia: are you ready for hotel hunting? We need to book something finally for Lisbon\\r\\nJuliette: sure we can go on, show me what you found\\r\\nJuliette: nah... it looks like an old lady's room lol\\r\\nJuliette: that's better... but the bed doesn't look very comfortable\\r\\nGeorgia: i kind of like it and it's really close to the city center\\r\\nJuliette: show me the others please\\r\\nJuliette: nah... this one sucks too, look at those horrible curtains \\r\\nGeorgia: aff Julie you are such a princess\\r\\nJuliette: i just want to be comfortable\\r\\nGeorgia: come on, stop whining you know we are on a budget\\r\\nJuliette: well hopefully we can find something that's decent right?\\r\\nGeorgia: i did show you decent but you want a Marriott or something :/\\r\\nJuliette: ok ok don't get angry\\r\\nGeorgia: we need to decide today, the longer we wait the higher the prices get \\r\\nJuliette: ok how about we get the second one then?\\r\\nGeorgia: ok give me a second\\r\\nJuliette: sure\\r\\nGeorgia: affff someon...</td>\n",
       "      <td>Georgia and Juliette are looking for a hotel in Lisbon. Juliette dislikes Georgia's choices. Juliette and Georgia decide on the second option presented by Georgia, but it has already been booked. Finally Georgia books the third hotel.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "14729  13819050   \n",
       "14730  13828395   \n",
       "14731  13729017   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      dialogue  \\\n",
       "14729                                                                                                                                     John: Every day some bad news. Japan will hunt whales again\\r\\nErica: Yes, I've read this. It's very upsetting\\r\\nJohn: Cruel Japanese\\r\\nFaith: I think this is a racist remark. Because Island and Norways has never joined this international whaling agreement\\r\\nErica: really? I haven't known, everybody is so outraged by Japan\\r\\nFaith: sure, European hypocrisy \\r\\nJohn: not entirely. Scandinavians don't use the nets that Japanese use, so Norway and Island kill much less specimens that Japan will\\r\\nFaith: oh, it's much more complex than one may expect\\r\\nJohn: True, but the truth is, that all of them should stop\\r\\nJohn: and this decision is a step back\\r\\nFaith: yes, this is worrying\\r\\nErica: And it seems that the most important whaling countries are out of the agreement right now\\r\\nFaith: yes, seems so\\r\\nJohn: Just like USA leaving the Paris Agreement   \n",
       "14730  Jennifer: Dear Celia! How are you doing?\\r\\nJennifer: The afternoon with the Collins was very pleasant, nice folks, but we missed you.\\r\\nJennifer: But I appreciate your consideration for Peter.\\r\\nCelia: My dear Jenny! It turns out that my decision not to come, though I wanted so much to see you again and Peter and the Collins, was right. Yesterday it all developed into a full bore cold. Sh.....\\r\\nCelia: All symptoms like in a text book.\\r\\nCelia: Luckily it's contagious only on the first 2, 3 days, so when we meet next week it should be alright.\\r\\nCelia: Thanks for asking! Somehow for all of us Peter comes first now.\\r\\nJennifer: That's too bad. Poor you...\\r\\nJennifer: I'll be driving to FR, do you want me to bring you sth? It's on my way.\\r\\nCelia: Thank you dear! I was at the pharmacy yesterday and had done my shopping the day before.\\r\\nCelia: You'd better still stay away from me in case I'm still contagious\\r\\nJennifer: Right. So I'll only leave a basket on your terrace. A...   \n",
       "14731  Georgia: are you ready for hotel hunting? We need to book something finally for Lisbon\\r\\nJuliette: sure we can go on, show me what you found\\r\\nJuliette: nah... it looks like an old lady's room lol\\r\\nJuliette: that's better... but the bed doesn't look very comfortable\\r\\nGeorgia: i kind of like it and it's really close to the city center\\r\\nJuliette: show me the others please\\r\\nJuliette: nah... this one sucks too, look at those horrible curtains \\r\\nGeorgia: aff Julie you are such a princess\\r\\nJuliette: i just want to be comfortable\\r\\nGeorgia: come on, stop whining you know we are on a budget\\r\\nJuliette: well hopefully we can find something that's decent right?\\r\\nGeorgia: i did show you decent but you want a Marriott or something :/\\r\\nJuliette: ok ok don't get angry\\r\\nGeorgia: we need to decide today, the longer we wait the higher the prices get \\r\\nJuliette: ok how about we get the second one then?\\r\\nGeorgia: ok give me a second\\r\\nJuliette: sure\\r\\nGeorgia: affff someon...   \n",
       "\n",
       "                                                                                                                                                                                                                                           summary  \n",
       "14729                                                                                                    Japan is going to hunt whales again. Island and Norway never stopped hunting them. The Scandinavians kill fewer whales than the Japanese.  \n",
       "14730         Celia couldn't make it to the afternoon with the Collins and Jennifer as she is ill. She's working, but doesn't want to meet with Jennifer as it might be contagious. Jennifer will leave a basket with cookies on Celia's terrace.   \n",
       "14731  Georgia and Juliette are looking for a hotel in Lisbon. Juliette dislikes Georgia's choices. Juliette and Georgia decide on the second option presented by Georgia, but it has already been booked. Finally Georgia books the third hotel.   "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(3) # Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c22716e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract dialogues and summaries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dialogues \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      3\u001b[0m train_summaries \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      4\u001b[0m val_dialogues \u001b[38;5;241m=\u001b[39m val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract dialogues and summaries\n",
    "train_dialogues = train['dialogue'].tolist()\n",
    "train_summaries = train['summary'].tolist()\n",
    "val_dialogues = val['dialogue'].tolist()\n",
    "val_summaries = val['summary'].tolist()\n",
    "\n",
    "# Fit tokenizer on both dialogues and summaries\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_dialogues + train_summaries + val_dialogues + val_summaries)\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_dialogue_seq = tokenizer.texts_to_sequences(train_dialogues)\n",
    "train_summary_seq = tokenizer.texts_to_sequences(train_summaries)\n",
    "val_dialogue_seq = tokenizer.texts_to_sequences(val_dialogues)\n",
    "val_summary_seq = tokenizer.texts_to_sequences(val_summaries)\n",
    "\n",
    "# Set max sequence lengths\n",
    "MAX_DIALOGUE_LEN = 256\n",
    "MAX_SUMMARY_LEN = 32\n",
    "\n",
    "# Pad or truncate sequences\n",
    "train_dialogue_seq = pad_sequences(train_dialogue_seq, maxlen=MAX_DIALOGUE_LEN, padding='post', truncating='post')\n",
    "train_summary_seq = pad_sequences(train_summary_seq, maxlen=MAX_SUMMARY_LEN, padding='post', truncating='post')\n",
    "val_dialogue_seq = pad_sequences(val_dialogue_seq, maxlen=MAX_DIALOGUE_LEN, padding='post', truncating='post')\n",
    "val_summary_seq = pad_sequences(val_summary_seq, maxlen=MAX_SUMMARY_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e95f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, dialogues, summaries):\n",
    "        self.dialogues = dialogues\n",
    "        self.summaries = summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = torch.tensor(self.dialogues[idx], dtype=torch.long)\n",
    "        summary = torch.tensor(self.summaries[idx], dtype=torch.long)\n",
    "        return dialogue, summary\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SummarizationDataset(train_dialogue_seq, train_summary_seq)\n",
    "val_dataset = SummarizationDataset(val_dialogue_seq, val_summary_seq)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5362ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of passing DataFrames, use the DataLoader to get batches of tokenized and padded tensors\n",
    "for batch_idx, (dialogue_batch, summary_batch) in enumerate(train_loader):\n",
    "    # dialogue_batch and summary_batch are already tensors of sequences\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Dialogue batch shape:\", dialogue_batch.shape)\n",
    "    print(\"Summary batch shape:\", summary_batch.shape)\n",
    "    break  # Remove this break to iterate through all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14843d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Update loss to CrossEntropyLoss for sequence prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# You can adjust as needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_index['<PAD>'])\n",
    "\n",
    "num_epochs = 20  # You can adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for dialogue_batch, summary_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: input is dialogue, target is summary\n",
    "        # For teacher forcing, pass summary_batch[:, :-1] as decoder input, predict summary_batch[:, 1:]\n",
    "        outputs = model(dialogue_batch, summary_batch[:, :-1])  # You need to update your model to accept decoder input\n",
    "        # outputs: (batch, seq_len, vocab_size)\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), summary_batch[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries from new dialogue sequences\n",
    "def generate_summary(model, dialogue_seq, tokenizer, max_summary_len=32, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor(dialogue_seq).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    summary_seq = [tokenizer.word_index.get('<START>', 1)]  # Start token\n",
    "    for _ in range(max_summary_len):\n",
    "        summary_tensor = torch.tensor(summary_seq).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, summary_tensor)\n",
    "            next_token = output.argmax(-1)[:, -1].item()\n",
    "        if next_token == tokenizer.word_index.get('<END>', 2) or next_token == 0:\n",
    "            break\n",
    "        summary_seq.append(next_token)\n",
    "    # Convert token ids back to words\n",
    "    inv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    summary_words = [inv_vocab.get(idx, '') for idx in summary_seq[1:]]\n",
    "    return ' '.join(summary_words)\n",
    "\n",
    "# Example usage:\n",
    "# new_dialogue = \"Alice: Hi Bob!\\nBob: Hello Alice, how are you?\"\n",
    "# new_seq = tokenizer.texts_to_sequences([new_dialogue])\n",
    "# new_seq = pad_sequences(new_seq, maxlen=256, padding='post', truncating='post')\n",
    "# print(generate_summary(model, new_seq[0], tokenizer))\n",
    "\n",
    "# Saving model checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, path='model_checkpoint.pth'):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, path)\n",
    "\n",
    "# Loading model checkpoint\n",
    "def load_checkpoint(model, optimizer, path='model_checkpoint.pth'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a520aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hn, cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
